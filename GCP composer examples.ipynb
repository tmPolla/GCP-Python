{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python bash\n",
    "# base very simple code\n",
    "\n",
    "import datetime\n",
    "\n",
    "from airflow import models\n",
    "from airflow.operators import bash_operator\n",
    "from airflow.operators import python_operator\n",
    "\n",
    "# \n",
    "yesterday = datetime.datetime.combine(\n",
    "    datetime.datetime.today() - datetime.timedelta(1),\n",
    "    datetime.datetime.min.time())\n",
    "\n",
    "# list of arguments\n",
    "default_dag_args = {\n",
    "    'start_date': yesterday\n",
    "}\n",
    "\n",
    "# init of DAG object, daily run\n",
    "with models.DAG(\n",
    "        'running_python_and_bash_operator',\n",
    "        schedule_interval=datetime.timedelta(days=1),\n",
    "        default_args=default_dag_args) as dag:\n",
    "\n",
    "    def hello_world():\n",
    "        print('Hello World!')\n",
    "        return 1\n",
    "\n",
    "    def greeting():\n",
    "        print('Greetings from SpikeySales! Happy shopping.')\n",
    "        return 'Greeting successfully printed.'\n",
    "    \n",
    "    # these are the 3 operators\n",
    "    # PythonOperator can call (UDF) functions\n",
    "    hello_world_greeting = python_operator.PythonOperator(\n",
    "        task_id='python_1',\n",
    "        python_callable=hello_world)\n",
    "    \n",
    "    spikeysales_greeting = python_operator.PythonOperator(\n",
    "        task_id='python_2',\n",
    "        python_callable=greeting)\n",
    "\n",
    "    bash_greeting = bash_operator.BashOperator(\n",
    "        task_id='bye_bash',\n",
    "        bash_command='echo Goodbye! Hope to see you soon.')\n",
    "    \n",
    "    #call of the operators\n",
    "    hello_world_greeting >> spikeysales_greeting >> bash_greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_success_python_bash.py\n",
    "# using trigger rule\n",
    "\n",
    "import datetime\n",
    "\n",
    "from airflow import models\n",
    "from airflow.operators import bash_operator\n",
    "from airflow.operators import python_operator\n",
    "from airflow.utils import trigger_rule\n",
    "\n",
    "# start day is yesterday\n",
    "yesterday = datetime.datetime.combine(\n",
    "    datetime.datetime.today() - datetime.timedelta(1),\n",
    "    datetime.datetime.min.time())\n",
    "\n",
    "# re-try once after 2 minutes\n",
    "default_dag_args = {\n",
    "    'start_date': yesterday,\n",
    "    'retries': 1,\n",
    "    'retry_delay': datetime.timedelta(minutes=2)\n",
    "}\n",
    "\n",
    "\n",
    "with models.DAG(\n",
    "        'python_and_bash_with_all_success_trigger',\n",
    "        schedule_interval=datetime.timedelta(days=1),\n",
    "        default_args=default_dag_args) as dag:\n",
    "\n",
    "    def hello_world():\n",
    "        raise ValueError('Oops! something went wrong.')\n",
    "        print('Hello World!')\n",
    "        return 1\n",
    "\n",
    "    def greeting():\n",
    "        print('Greetings from SpikeySales! Happy shopping.')\n",
    "        return 'Greeting successfully printed.'\n",
    "\n",
    "    hello_world_greeting = python_operator.PythonOperator(\n",
    "        task_id='python_1',\n",
    "        python_callable=hello_world)\n",
    "\n",
    "    spikeysales_greeting = python_operator.PythonOperator(\n",
    "        task_id='python_2',\n",
    "        python_callable=greeting)\n",
    "\n",
    "    # here you can find the trigger rule\n",
    "    bash_greeting = bash_operator.BashOperator(\n",
    "        task_id='bye_bash',\n",
    "        bash_command='echo Goodbye! Hope to see you soon.',\n",
    "        trigger_rule=trigger_rule.TriggerRule.ALL_SUCCESS)\n",
    "\n",
    "    hello_world_greeting >> spikeysales_greeting >> bash_greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_failed_python_bash.py\n",
    "\n",
    "import datetime\n",
    "\n",
    "from airflow import models\n",
    "from airflow.operators import bash_operator\n",
    "from airflow.operators import python_operator\n",
    "from airflow.utils import trigger_rule\n",
    "\n",
    "yesterday = datetime.datetime.combine(\n",
    "    datetime.datetime.today() - datetime.timedelta(1),\n",
    "    datetime.datetime.min.time())\n",
    "\n",
    "default_dag_args = {\n",
    "    'start_date': yesterday,\n",
    "    'retries': 1,\n",
    "    'retry_delay': datetime.timedelta(minutes=2)\n",
    "}\n",
    "\n",
    "\n",
    "with models.DAG(\n",
    "        'python_and_bash_with_one_failed_trigger',\n",
    "        schedule_interval=datetime.timedelta(days=1),\n",
    "        default_args=default_dag_args) as dag:\n",
    "\n",
    "    def hello_world():\n",
    "        raise ValueError('Oops! something went wrong.')\n",
    "        print('Hello World!')\n",
    "        return 1\n",
    "\n",
    "    def greeting():\n",
    "        print('Greetings from SpikeySales! Happy shopping.')\n",
    "        return 'Greeting successfully printed.'\n",
    "\n",
    "    hello_world_greeting = python_operator.PythonOperator(\n",
    "        task_id='python_1',\n",
    "        python_callable=hello_world)\n",
    "    \n",
    "    spikeysales_greeting = python_operator.PythonOperator(\n",
    "        task_id='python_2',\n",
    "        python_callable=greeting)\n",
    "\n",
    "    bash_greeting = bash_operator.BashOperator(\n",
    "        task_id='bye_bash',\n",
    "        bash_command='echo Goodbye! Hope to see you soon.',\n",
    "        trigger_rule=trigger_rule.TriggerRule.ONE_FAILED)\n",
    "\n",
    "    hello_world_greeting >> spikeysales_greeting >> bash_greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all done trigger type\n",
    "# raise error in every step\n",
    "\n",
    "import datetime\n",
    "\n",
    "from airflow import models\n",
    "from airflow.operators import bash_operator\n",
    "from airflow.operators import python_operator\n",
    "from airflow.utils import trigger_rule\n",
    "\n",
    "yesterday = datetime.datetime.combine(\n",
    "    datetime.datetime.today() - datetime.timedelta(1),\n",
    "    datetime.datetime.min.time())\n",
    "\n",
    "default_dag_args = {\n",
    "    'start_date': yesterday,\n",
    "    'retries': 1,\n",
    "    'retry_delay': datetime.timedelta(minutes=2)\n",
    "}\n",
    "\n",
    "\n",
    "with models.DAG(\n",
    "        'python_and_bash_with_all_done_trigger',\n",
    "        schedule_interval=datetime.timedelta(days=1),\n",
    "        default_args=default_dag_args) as dag:\n",
    "\n",
    "    def hello_world():\n",
    "        raise ValueError('Oops! something went wrong.')\n",
    "        print('Hello World!')\n",
    "        return 1\n",
    "\n",
    "    def greeting():\n",
    "        raise TypeError('Incorrect type.')\n",
    "        print('Greetings from SpikeySales! Happy shopping.')\n",
    "        return 'Greeting successfully printed.'\n",
    "\n",
    "    hello_world_greeting = python_operator.PythonOperator(\n",
    "        task_id='python_1',\n",
    "        python_callable=hello_world)\n",
    "    \n",
    "\n",
    "    spikeysales_greeting = python_operator.PythonOperator(\n",
    "        task_id='python_2',\n",
    "        python_callable=greeting)\n",
    "\n",
    "    bash_greeting = bash_operator.BashOperator(\n",
    "        task_id='bye_bash',\n",
    "        bash_command='echo Goodbye! Hope to see you soon.',\n",
    "        trigger_rule=trigger_rule.TriggerRule.ALL_DONE)\n",
    "\n",
    "    hello_world_greeting >> spikeysales_greeting >> bash_greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with stats\n",
    "# if you just simple upload and try to run the DAG it will fail because\n",
    "# the airflow doesn't know the scipy, we have to set in the composer / PYPI\n",
    "# packages and edit and write \"scipy>=1.1.0\"\n",
    "\n",
    "import datetime\n",
    "from scipy import stats\n",
    "\n",
    "from airflow import models\n",
    "from airflow.operators import python_operator\n",
    "\n",
    "yesterday = datetime.datetime.combine(\n",
    "    datetime.datetime.today() - datetime.timedelta(1),\n",
    "    datetime.datetime.min.time())\n",
    "\n",
    "default_dag_args = {\n",
    "    'start_date': yesterday,\n",
    "    'retries': 1,\n",
    "    'retry_delay': datetime.timedelta(minutes=2)\n",
    "}\n",
    "\n",
    "with models.DAG(\n",
    "        'finding_the_most_common_element',\n",
    "        schedule_interval=datetime.timedelta(days=1),\n",
    "        default_args=default_dag_args) as dag:\n",
    "\n",
    "    def print_most_common_number():\n",
    "        num = stats.mode([\"9\",\"5\",\"2\",\"5\",\"1\",\"6\"])\n",
    "        print(num)\n",
    "        return('Successfully printed most common element!')\n",
    "\n",
    "\n",
    "    printing_most_common_element = python_operator.PythonOperator(\n",
    "        task_id='most_common_number',\n",
    "        python_callable=print_most_common_number)\n",
    "\n",
    "    printing_most_common_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useing dummy\n",
    "\n",
    "import datetime\n",
    "\n",
    "from airflow import models\n",
    "from airflow.operators import bash_operator\n",
    "from airflow.operators import python_operator\n",
    "from airflow.operators import dummy_operator\n",
    "\n",
    "yesterday = datetime.datetime.combine(\n",
    "    datetime.datetime.today() - datetime.timedelta(1),\n",
    "    datetime.datetime.min.time())\n",
    "\n",
    "default_dag_args = {\n",
    "    'start_date': yesterday\n",
    "}\n",
    "\n",
    "\n",
    "with models.DAG(\n",
    "        'running_python_bash_and_dummy_operator',\n",
    "        schedule_interval=datetime.timedelta(days=1),\n",
    "        default_args=default_dag_args) as dag:\n",
    "\n",
    "    def hello_world():\n",
    "        print('Hello World!')\n",
    "        return 1\n",
    "\n",
    "    def greeting():\n",
    "        print('Greetings from SpikeySales! Happy shopping.')\n",
    "        return 'Greeting successfully printed.'\n",
    "\n",
    "    hello_world_greeting = python_operator.PythonOperator(\n",
    "        task_id='python_1',\n",
    "        python_callable=hello_world)\n",
    "    \n",
    "    spikeysales_greeting = python_operator.PythonOperator(\n",
    "        task_id='python_2',\n",
    "        python_callable=greeting)\n",
    "\n",
    "    bash_greeting = bash_operator.BashOperator(\n",
    "        task_id='bye_bash',\n",
    "        bash_command='echo Goodbye! Hope to see you soon.')\n",
    "\n",
    "    end = dummy_operator.DummyOperator(\n",
    "        task_id='dummy')\n",
    "\n",
    "    hello_world_greeting >> spikeysales_greeting >> bash_greeting >> end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make choice if the random number is small than \n",
    "# hello else dummy variable\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "from airflow import models\n",
    "from airflow.operators import bash_operator\n",
    "from airflow.operators import python_operator\n",
    "from airflow.operators import dummy_operator\n",
    "\n",
    "yesterday = datetime.datetime.combine(\n",
    "    datetime.datetime.today() - datetime.timedelta(1),\n",
    "    datetime.datetime.min.time())\n",
    "\n",
    "default_dag_args = {\n",
    "    'start_date': yesterday\n",
    "}\n",
    "\n",
    "with models.DAG(\n",
    "        'branching_python_operator',\n",
    "        schedule_interval=datetime.timedelta(days=1),\n",
    "        default_args=default_dag_args) as dag:\n",
    "\n",
    "    def greeting():\n",
    "        print('Greetings from SpikeySales! Happy shopping.')\n",
    "        return 'Greeting successfully printed.' \n",
    "\n",
    "    def makeBranchChoice():\n",
    "        x = random.randint(1, 5)\n",
    "\n",
    "        if(x <= 2):\n",
    "            return 'hello_spikey'\n",
    "\n",
    "        else:\n",
    "            return 'dummy'  \n",
    "\n",
    "    run_this_first = dummy_operator.DummyOperator(\n",
    "        task_id='run_this_first'\n",
    "    )\n",
    "\n",
    "    branching = python_operator.BranchPythonOperator(\n",
    "        task_id='branching',\n",
    "        python_callable=makeBranchChoice\n",
    "    )\n",
    "\n",
    "    run_this_first >> branching\n",
    "          \n",
    "    spikeysales_greeting = python_operator.PythonOperator(\n",
    "        task_id='hello_spikey',\n",
    "        python_callable=greeting)\n",
    "\n",
    "    dummy_followed_python = dummy_operator.DummyOperator(\n",
    "        task_id='follow_python')\n",
    "\n",
    "    dummy = dummy_operator.DummyOperator(\n",
    "        task_id='dummy')\n",
    "\n",
    "    bash_greeting = bash_operator.BashOperator(\n",
    "        task_id='bye_bash',\n",
    "        bash_command='echo Goodbye! Hope to see you soon.',\n",
    "        trigger_rule='one_success'\n",
    "    )\n",
    "\n",
    "    branching >> spikeysales_greeting >> dummy_followed_python >> bash_greeting\n",
    "    branching >> dummy >> bash_greeting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run query via big query\n",
    "# send email notification via send grid\n",
    "# before you can use Send Grid you have to set up the\n",
    "# send grid API!\n",
    "\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from airflow import models\n",
    "from airflow.contrib.operators import bigquery_operator\n",
    "from airflow.contrib.operators import bigquery_to_gcs\n",
    "from airflow.operators import email_operator\n",
    "from airflow.contrib.operators import bigquery_table_delete_operator\n",
    "from airflow.utils import trigger_rule\n",
    "\n",
    "updated_time = time.strftime('%d-%m-%Y %H:%M:%S')\n",
    "\n",
    "bq_dataset_name = models.Variable.get('bq_dataset_name')\n",
    "bq_products_on_sale_table_id = bq_dataset_name + '.temp_sale_table'\n",
    "output_file = '{gcs_bucket}/products_on_sale {current_time}.csv'.format(\n",
    "    gcs_bucket=models.Variable.get('gcs_bucket'), current_time=updated_time)\n",
    "\n",
    "yesterday = datetime.datetime.combine(\n",
    "    datetime.datetime.today() - datetime.timedelta(1),\n",
    "    datetime.datetime.min.time())\n",
    "\n",
    "email_id = models.Variable.get('email')\n",
    "\n",
    "default_dag_args = {\n",
    "    'start_date': yesterday,\n",
    "    'email': email_id,\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': datetime.timedelta(minutes=2),\n",
    "    'project_id': models.Variable.get('gcp_project')\n",
    "}\n",
    "\n",
    "with models.DAG(\n",
    "        'current_products_on_sale_notification',\n",
    "        schedule_interval=datetime.timedelta(days=1),\n",
    "        default_args=default_dag_args) as dag:\n",
    "\n",
    "    query_current_sales_products = bigquery_operator.BigQueryOperator(\n",
    "        task_id='query_products_on_sale',\n",
    "        bql=\"\"\"\n",
    "        SELECT product_id, product_name\n",
    "        FROM `{bq_table_id}` \n",
    "        WHERE sale = True\n",
    "        \"\"\".format(bq_table_id=models.Variable.get('bq_table_id')),\n",
    "        use_legacy_sql=False,\n",
    "        destination_dataset_table=bq_products_on_sale_table_id,\n",
    "        write_disposition='WRITE_TRUNCATE')\n",
    "\n",
    "    export_data_to_gcs = bigquery_to_gcs.BigQueryToCloudStorageOperator(\n",
    "        task_id='export_sale_data_to_gcs',\n",
    "        source_project_dataset_table=bq_products_on_sale_table_id,\n",
    "        destination_cloud_storage_uris=[output_file],\n",
    "        export_format='CSV')\n",
    "\n",
    "    email_updation_notification = email_operator.EmailOperator(\n",
    "        task_id='email_notification',\n",
    "        to=email_id,\n",
    "        subject='Sale product data updated',\n",
    "        html_content=\"\"\"\n",
    "        Updated sale products for {current_time}.\n",
    "        \"\"\".format(current_time=updated_time),\n",
    "        trigger_rule=trigger_rule.TriggerRule.ALL_SUCCESS)\n",
    "\n",
    "    delete_bq_table = bigquery_table_delete_operator.BigQueryTableDeleteOperator(\n",
    "        task_id='delete_bigquery_table',\n",
    "        deletion_dataset_table=bq_products_on_sale_table_id)\n",
    "    ( \n",
    "    query_current_sales_products \n",
    "    >> export_data_to_gcs \n",
    "    >> email_updation_notification \n",
    "    >> delete_bq_table\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with hadoop based dataproc\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from airflow import models\n",
    "from airflow.contrib.operators import dataproc_operator\n",
    "from airflow.utils import trigger_rule\n",
    "\n",
    "output_file = os.path.join(\n",
    "    models.Variable.get('gcs_bucket'), 'wordcount',\n",
    "    datetime.datetime.now().strftime('%Y%m%d-%H%M%S')) + os.sep\n",
    "\n",
    "WORDCOUNT_JAR = (\n",
    "    'file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'\n",
    ")\n",
    "\n",
    "wordcount_args = ['wordcount', 'gs://us-central1-spikey-composer-486ba46f-bucket/dags/doc.txt', output_file]\n",
    "\n",
    "yesterday = datetime.datetime.combine(\n",
    "    datetime.datetime.today() - datetime.timedelta(1),\n",
    "    datetime.datetime.min.time())\n",
    "\n",
    "default_dag_args = {\n",
    "\n",
    "    'start_date': yesterday,\n",
    "\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "\n",
    "    'retries': 1,\n",
    "    'retry_delay': datetime.timedelta(minutes=5),\n",
    "    'project_id': models.Variable.get('gcp_project')\n",
    "}\n",
    "\n",
    "\n",
    "with models.DAG(\n",
    "        'composer_hadoop_wordcount',\n",
    "\n",
    "        schedule_interval=datetime.timedelta(days=1),\n",
    "        default_args=default_dag_args) as dag:\n",
    "\n",
    "    create_dataproc_cluster = dataproc_operator.DataprocClusterCreateOperator(\n",
    "        task_id='create_dataproc_cluster',\n",
    "        \n",
    "        cluster_name='spikey-wordcount-cluster-{{ ds_nodash }}',\n",
    "        num_workers=2,\n",
    "        zone=models.Variable.get('gce_zone'),\n",
    "        master_machine_type='n1-standard-1',\n",
    "        worker_machine_type='n1-standard-1')\n",
    "\n",
    "\n",
    "    run_dataproc_hadoop = dataproc_operator.DataProcHadoopOperator(\n",
    "        task_id='run_dataproc_hadoop',\n",
    "        main_jar=WORDCOUNT_JAR,\n",
    "        cluster_name='spikey-wordcount-cluster-{{ ds_nodash }}',\n",
    "        arguments=wordcount_args)\n",
    "\n",
    "\n",
    "    delete_dataproc_cluster = dataproc_operator.DataprocClusterDeleteOperator(\n",
    "        task_id='delete_dataproc_cluster',\n",
    "        cluster_name='spikey-wordcount-cluster-{{ ds_nodash }}',\n",
    "\n",
    "        trigger_rule=trigger_rule.TriggerRule.ALL_DONE)\n",
    "\n",
    "    create_dataproc_cluster >> run_dataproc_hadoop >> delete_dataproc_cluster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
